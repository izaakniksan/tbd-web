<!DOCTYPE html>
<html lang="en">

<head>

	<title>FP32 Utilization Comparison | TBD</title>
	
	<meta charset="utf-8">
	<meta name="viewpoint" content="width=device-width, initial-scale=1">
	<link rel="icon" href="./images/uoft.png">
	
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" 
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="ecosystem.css">
	<link rel="stylesheet" type="text/css" href="tbd.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

	<script src="https://www.w3schools.com/lib/w3.js"></script>
	<script src="https://use.fontawesome.com/fd789ca0f7.js"></script>

</head>

<body>

	<div w3-include-html="./components/tbd-titlebar.html"></div>

	<main id="content" role="main" class="doc">
		<div class="container">
		  <br><br>
		  <h2>Hardware Sensitivity</h2>
		  <br>
			<ul>
			<li class="toctree-l1"><a class="reference internal" href="end2end.html">End-to-End Training</a></li>
			<li class="toctree-l1"><a class="reference internal" href="throughput.html">FP32 Throughput</a></li>
			<li class="toctree-l1"><a class="reference internal" href="compute_utilization.html">Compute Utilization</a></li>
			<li class="toctree-l1"><a class="reference internal" href="fp32_utilization.html">FP32 Utilization</a></li>
			<li class="toctree-l1">FP16 Throughput</a></li>
			<li class="toctree-l1"><a class="reference internal" href="specs.html">Hardware Specifications</a></li>
			<!--
			<li class="toctree-l1"><a class="reference internal" href="fp16_utilization.html">FP16 Throughput</a></li>
			-->
			</ul>
			<br>
			<h3>FP32 Utilization</h3>
			<p>Single precision floats are commonly used by machine learning practitioners to ensure the model accuracy is not corrupted. We define the FP32 utilization as the ratio between average number of FLOPS executed per cycle over the maximum number of FLOPS executed per cycle. This metric indicates how well the single precision float cores are utilized by DNN workloads. A low FP32 utilization indicates the potential performance gain that can be achieved if the software libraries (e.g. cuda, cudnn, mkl, etc.) can be further improved.</p>
			
			<p>Currently we present only the results of GPUs. We will gradually present more results as more hardware accelerator resources become available.</p>

			<p>The following figure shows the FP32 core utilization results for different benchmarks running on various GPUs. These results are based on cuda 9, cudnn 7 and Ubuntu 16.04 LTE OS.</p>
			<div class="margin-10">
				<img class="middle_img" width="700" src="./data/fp32_utilization.png"/>
				<div class="text-align-center">The FP32 utilization of different benchmarks on 4 GPUs</div>
				<br>
			</div>

			<p>We can make the following observations:</p>
			<ul>
			<li class="toctree-l1">Image classification models and the transformer model are the most friendly benchmarks for GPUs in terms of FP32 utilization. Other models have FP32 utilization of 70% at most, and it is worse for RNN models.</li>
			<li class="toctree-l1">Similar to compute utilization, the general trend is that FP32 utilization drops as GPUs become more powerful.</li>
			<li class="toctree-l1">From the last observation, we can conclude that as hardware accelerators become faster, there are increasing potentials in optimizating software libraries. Stronger hardware accelerators should be equipped with better software libraries so that the computation power can be fully utilized.</li>
			</ul>
		<br><br>

		</div>
	</main>

	<script>w3.includeHTML();</script>

	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" 
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" 
		crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" 
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" 
		crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" 
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" 
		crossorigin="anonymous"></script>

</body>
</html>
