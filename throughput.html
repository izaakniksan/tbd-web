<!DOCTYPE html>
<html lang="en">

<head>

	<title>GPU Comparison | TBD</title>
	
	<meta charset="utf-8">
	<meta name="viewpoint" content="width=device-width, initial-scale=1">
	<link rel="icon" href="./images/uoft.png">
	
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" 
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="ecosystem.css">
	<link rel="stylesheet" type="text/css" href="tbd.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

	<script src="https://www.w3schools.com/lib/w3.js"></script>
	<script src="https://use.fontawesome.com/fd789ca0f7.js"></script>

</head>

<body>

	<div w3-include-html="./components/tbd-titlebar.html"></div>

	<main id="content" role="main" class="doc">
		<div class="container">
		  <br><br>
		  <h2>Hardware Sensitivity</h2>
		  <br>
			<ul>
			<li class="toctree-l1"><a class="reference internal" href="end2end.html">End-to-End Training</a></li>
			<li class="toctree-l1"><a class="reference internal" href="throughput.html">FP32 Throughput</a></li>
			<li class="toctree-l1">GPU Compute Utilization</li>
			<li class="toctree-l1">GPU FP32 Utilization</a></li>
			<li class="toctree-l1">FP16 Throughput</a></li>
			<li class="toctree-l1"><a class="reference internal" href="specs.html">GPU Specifications</a></li>
			<!--
			<li class="toctree-l1"><a class="reference internal" href="compute_utilization.html">GPU Compute Utilization</a></li>
			<li class="toctree-l1"><a class="reference internal" href="fp32_utilization.html">GPU FP32 Utilization</a></li>
			<li class="toctree-l1"><a class="reference internal" href="fp16_utilization.html">FP16 Throughput</a></li>
			-->
			</ul>

			<div class="container">
				<br>
				<div class="section" id="sensitivity">
					<h3>FP32 Throughput Comparison</h3>
					<br>
					<p>We show the throughput results on different GPU architectures including the latest Pascal & Volta generations. Unlike the baseline analysis on P4000, we use a fixed mini-batch size for each benchmark. All benchmarks are trained in single-precision floats.</p>
					<div class="margin-10">
						<img width="500" src="./data/throughput_resnet.png"/>
						<img width="500" src="./data/throughput_inception.png"/>
						<br>
					</div>
					<p>As we can see that for image classification benchmarks, V100-SXM2 outperforms all other GPU architectures. The second tier includes the V100-PCIe, 2080 Ti and Titan V. Their performance is roughly 15% slower than V100-SXM2. The Pascal architectures deliver similar performance comparing to 1080 Ti, which is slightly (10%) less than Titan Xp. We observed similar performance tiers for Faster RCNN benchmark, as Faster RCNN uses an image classification model as its base model.</p>
					<div class="margin-10">
						<img class="middle_img" width="500" src="./data/throughput_frcnn.png"/>
						<br>
					</div>
					<p>For the seq2seq model, The V100 architectures are still able to deliver the best training speed. However for the sockeye implementation, V100-PCIe is only faster than P100 and Quadro P4000, and Titan Xp and 1080 Ti are much more competative. For the NMT benchmark, the V100-PCIe architecture is slightly faster than V100-SXM2.</p>
					<div class="margin-10">
						<img width="500" src="./data/throughput_sockeye.png"/>
						<img width="500" src="./data/throughput_nmt.png"/>
						<br>
					</div>
					<p>For the transformer model, the performance tiers are the same as image classification models. The only difference is that the Titan Xp, 1080 Ti, P100 and P4000 are much slower than (roughly 50% slower) others. For image classification they are also around 30% slower.</p>
					<div class="margin-10">
						<img class="middle_img" width="500" src="./data/throughput_transformer.png"/>
						<br>
					</div>
					<p>The A3C model uses a relatively shallow CNN network as its base model. In this case the V100 architectures are not able to outperform other architectures.</p>
					<div class="margin-10">
						<img class="middle_img" width="500" src="./data/throughput_a3c.png"/>
						<br>
					</div>
					<p>In general, we conclude that the V100 architectures deliver the best DNN training performance. The Titan V and 2080 Ti architectures are equally fast and competative (only 15% slower than V100 on average). Titan Xp is slightly (10%) faster than 1080 Ti, but slower than Titan V and 2080 Ti. The P100 architectures are much slower than others and only faster than our baseline Quadro P4000.</p>
				</div>
			</div>
					
		<br><br>

		</div>
        <br/><br/>
	</main>

	<script>w3.includeHTML();</script>

	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" 
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" 
		crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" 
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" 
		crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" 
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" 
		crossorigin="anonymous"></script>

</body>
</html>
